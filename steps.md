# üéØ –ü–æ—à–∞–≥–æ–≤—ã–π –ø–ª–∞–Ω –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ YOLOv10 + DINOv2

**–í–∞—à–∏ –¥–∞–Ω–Ω—ã–µ:** 10,000 —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö + 300,000 –Ω–µ—Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π

---

## üìã **–≠–¢–ê–ü 0: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö (1-2 –¥–Ω—è)**

### –®–∞–≥ 0.1: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
```bash
# –û—Å–Ω–æ–≤–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
pip install torch torchvision ultralytics
pip install transformers timm
pip install opencv-python albumentations
pip install scikit-learn matplotlib seaborn

# –î–ª—è DINOv2
pip install git+https://github.com/facebookresearch/dinov2.git
```

### –®–∞–≥ 0.2: –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
```python
import os
import cv2
import numpy as np
from collections import Counter
import matplotlib.pyplot as plt

def analyze_dataset(image_folder, annotation_folder=None):
    """–ê–Ω–∞–ª–∏–∑ –≤–∞—à–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    
    # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    image_stats = {
        'total_images': 0,
        'resolutions': [],
        'aspect_ratios': [],
        'file_sizes': []
    }
    
    for img_file in os.listdir(image_folder):
        if img_file.endswith(('.jpg', '.jpeg', '.png')):
            img_path = os.path.join(image_folder, img_file)
            img = cv2.imread(img_path)
            
            if img is not None:
                h, w = img.shape[:2]
                image_stats['total_images'] += 1
                image_stats['resolutions'].append((w, h))
                image_stats['aspect_ratios'].append(w/h)
                image_stats['file_sizes'].append(os.path.getsize(img_path))
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–º–µ—Ç–∫–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
    if annotation_folder:
        class_distribution = analyze_annotations(annotation_folder)
        return image_stats, class_distribution
    
    return image_stats, None

def analyze_annotations(annotation_folder):
    """–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤"""
    class_counts = Counter()
    bbox_sizes = []
    
    for ann_file in os.listdir(annotation_folder):
        if ann_file.endswith('.txt'):  # YOLO —Ñ–æ—Ä–º–∞—Ç
            with open(os.path.join(annotation_folder, ann_file), 'r') as f:
                for line in f:
                    parts = line.strip().split()
                    if len(parts) >= 5:
                        class_id = int(parts[0])
                        w, h = float(parts[3]), float(parts[4])
                        
                        class_counts[class_id] += 1
                        bbox_sizes.append(w * h)
    
    return {
        'class_distribution': dict(class_counts),
        'bbox_sizes': bbox_sizes,
        'total_objects': sum(class_counts.values())
    }

# –í—ã–ø–æ–ª–Ω–∏—Ç–µ –∞–Ω–∞–ª–∏–∑ –≤–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö
labeled_stats, class_info = analyze_dataset(
    "path/to/labeled/images", 
    "path/to/annotations"
)
unlabeled_stats, _ = analyze_dataset("path/to/unlabeled/images")

print(f"–†–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {labeled_stats['total_images']}")
print(f"–ù–µ—Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {unlabeled_stats['total_images']}")
print(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {class_info['class_distribution']}")
```

---

## üìä **–≠–¢–ê–ü 1: Baseline –æ–±—É—á–µ–Ω–∏–µ YOLOv10 (2-3 –¥–Ω—è)**

### –®–∞–≥ 1.1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è YOLOv10
```python
import yaml
from sklearn.model_selection import train_test_split

def prepare_yolo_dataset(image_folder, annotation_folder, output_folder):
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ YOLOv10"""
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø–∞–ø–æ–∫
    os.makedirs(f"{output_folder}/images/train", exist_ok=True)
    os.makedirs(f"{output_folder}/images/val", exist_ok=True)
    os.makedirs(f"{output_folder}/labels/train", exist_ok=True)
    os.makedirs(f"{output_folder}/labels/val", exist_ok=True)
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤
    image_files = [f for f in os.listdir(image_folder) if f.endswith(('.jpg', '.png'))]
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val (80/20)
    train_files, val_files = train_test_split(image_files, test_size=0.2, random_state=42)
    
    # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
    import shutil
    
    for split, files in [('train', train_files), ('val', val_files)]:
        for img_file in files:
            # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            src_img = os.path.join(image_folder, img_file)
            dst_img = os.path.join(output_folder, f"images/{split}", img_file)
            shutil.copy2(src_img, dst_img)
            
            # –ê–Ω–Ω–æ—Ç–∞—Ü–∏—è
            ann_file = img_file.replace('.jpg', '.txt').replace('.png', '.txt')
            src_ann = os.path.join(annotation_folder, ann_file)
            dst_ann = os.path.join(output_folder, f"labels/{split}", ann_file)
            if os.path.exists(src_ann):
                shutil.copy2(src_ann, dst_ann)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥–∞
    config = {
        'train': f"{output_folder}/images/train",
        'val': f"{output_folder}/images/val",
        'nc': len(class_info['class_distribution']),  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤
        'names': list(range(len(class_info['class_distribution'])))  # –∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ –∏–º–µ–Ω–∞
    }
    
    with open(f"{output_folder}/dataset.yaml", 'w') as f:
        yaml.dump(config, f)
    
    return f"{output_folder}/dataset.yaml"

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
dataset_config = prepare_yolo_dataset(
    "path/to/labeled/images",
    "path/to/annotations", 
    "prepared_dataset"
)
```

### –®–∞–≥ 1.2: Baseline –æ–±—É—á–µ–Ω–∏–µ YOLOv10
```python
from ultralytics import YOLO

def train_baseline_yolo():
    """–û–±—É—á–µ–Ω–∏–µ baseline YOLOv10"""
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
    model = YOLO('yolov10s.pt')  # –∏–ª–∏ yolov10m.pt –¥–ª—è –ª—É—á—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
    
    # –û–±—É—á–µ–Ω–∏–µ
    results = model.train(
        data=dataset_config,
        epochs=100,
        imgsz=640,
        batch=16,  # –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–æ–¥ –≤–∞—à—É GPU
        device=0,  # GPU
        name='baseline_yolov10',
        patience=20,  # early stopping
        save_period=10,  # —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –∫–∞–∂–¥—ã–µ 10 —ç–ø–æ—Ö
        
        # –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
        hsv_h=0.015,
        hsv_s=0.7,
        hsv_v=0.4,
        degrees=0.0,
        translate=0.1,
        scale=0.5,
        shear=0.0,
        perspective=0.0,
        flipud=0.0,
        fliplr=0.5,
        mosaic=1.0,
        mixup=0.0,
    )
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è
    val_results = model.val()
    
    print(f"Baseline mAP@0.5: {val_results.box.map50:.3f}")
    print(f"Baseline mAP@0.5:0.95: {val_results.box.map:.3f}")
    
    return model, val_results

# –û–±—É—á–µ–Ω–∏–µ baseline –º–æ–¥–µ–ª–∏
baseline_model, baseline_metrics = train_baseline_yolo()
```

---

## üîß **–≠–¢–ê–ü 2: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ DINOv2 (1 –¥–µ–Ω—å)**

### –®–∞–≥ 2.1: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
```python
import torch
from torch.utils.data import DataLoader, Dataset
from PIL import Image
import torchvision.transforms as transforms

class ImageDataset(Dataset):
    """–î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    def __init__(self, image_folder, transform=None):
        self.image_folder = image_folder
        self.image_files = [f for f in os.listdir(image_folder) 
                           if f.endswith(('.jpg', '.png'))]
        
        self.transform = transform or transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
    
    def __len__(self):
        return len(self.image_files)
    
    def __getitem__(self, idx):
        img_path = os.path.join(self.image_folder, self.image_files[idx])
        image = Image.open(img_path).convert('RGB')
        
        if self.transform:
            image = self.transform(image)
        
        return image, self.image_files[idx]

def extract_dino_features(image_folder, output_file, batch_size=32):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ DINOv2 –¥–ª—è –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ DINOv2
    dino_model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')
    dino_model.eval()
    dino_model = dino_model.cuda()
    
    # –î–∞—Ç–∞—Å–µ—Ç –∏ –¥–∞—Ç–∞–ª–æ–∞–¥–µ—Ä
    dataset = ImageDataset(image_folder)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)
    
    features_dict = {}
    
    print(f"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {len(dataset)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
    
    with torch.no_grad():
        for batch_idx, (images, filenames) in enumerate(dataloader):
            images = images.cuda()
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            features = dino_model(images)  # [batch_size, 768]
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Å–ª–æ–≤–∞—Ä—å
            for feat, filename in zip(features, filenames):
                features_dict[filename] = feat.cpu().numpy()
            
            if batch_idx % 100 == 0:
                print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –±–∞—Ç—á–µ–π: {batch_idx}/{len(dataloader)}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    np.save(output_file, features_dict)
    print(f"–ü—Ä–∏–∑–Ω–∞–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")
    
    return features_dict

# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
labeled_features = extract_dino_features(
    "path/to/labeled/images", 
    "labeled_dino_features.npy"
)

# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –Ω–µ—Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è!)
unlabeled_features = extract_dino_features(
    "path/to/unlabeled/images", 
    "unlabeled_dino_features.npy"
)
```

### –®–∞–≥ 2.2: –ê–Ω–∞–ª–∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–æ–º–µ–Ω–∞
```python
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

def analyze_domain_gap(labeled_features, unlabeled_features, sample_size=5000):
    """–ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–ª–∏—á–∏–π –º–µ–∂–¥—É —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–º–∏ –∏ –Ω–µ—Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏"""
    
    # –°–ª—É—á–∞–π–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    labeled_sample = np.random.choice(
        list(labeled_features.values()), 
        min(sample_size//2, len(labeled_features)), 
        replace=False
    )
    unlabeled_sample = np.random.choice(
        list(unlabeled_features.values()), 
        min(sample_size//2, len(unlabeled_features)), 
        replace=False
    )
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    all_features = np.vstack([
        np.stack(labeled_sample),
        np.stack(unlabeled_sample)
    ])
    
    labels = np.array(['labeled'] * len(labeled_sample) + 
                     ['unlabeled'] * len(unlabeled_sample))
    
    # PCA –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    pca = PCA(n_components=2)
    features_2d = pca.fit_transform(all_features)
    
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    for label in ['labeled', 'unlabeled']:
        mask = labels == label
        plt.scatter(features_2d[mask, 0], features_2d[mask, 1], 
                   alpha=0.6, label=label)
    plt.title('PCA –ø—Ä–æ–µ–∫—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ DINOv2')
    plt.legend()
    
    # t-SNE –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    tsne = TSNE(n_components=2, random_state=42)
    features_tsne = tsne.fit_transform(all_features)
    
    plt.subplot(1, 2, 2)
    for label in ['labeled', 'unlabeled']:
        mask = labels == label
        plt.scatter(features_tsne[mask, 0], features_tsne[mask, 1], 
                   alpha=0.6, label=label)
    plt.title('t-SNE –ø—Ä–æ–µ–∫—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ DINOv2')
    plt.legend()
    
    plt.tight_layout()
    plt.savefig('domain_analysis.png', dpi=150)
    plt.show()
    
    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ domain gap
    from scipy.spatial.distance import cdist
    
    labeled_centroid = np.mean(np.stack(labeled_sample), axis=0)
    unlabeled_centroid = np.mean(np.stack(unlabeled_sample), axis=0)
    
    domain_gap = np.linalg.norm(labeled_centroid - unlabeled_centroid)
    
    print(f"Domain gap (Euclidean distance): {domain_gap:.3f}")
    
    return domain_gap

# –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–ª–∏—á–∏–π –º–µ–∂–¥—É –¥–æ–º–µ–Ω–∞–º–∏
domain_gap = analyze_domain_gap(labeled_features, unlabeled_features)
```

---

## üéØ **–≠–¢–ê–ü 3: –ü—Å–µ–≤–¥–æ-—Ä–∞–∑–º–µ—Ç–∫–∞ —Å DINOv2 (2-3 –¥–Ω—è)**

### –®–∞–≥ 3.1: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞—á–∞–ª—å–Ω—ã—Ö –ø—Å–µ–≤–¥–æ-–º–µ—Ç–æ–∫
```python
def generate_initial_pseudo_labels(baseline_model, unlabeled_folder, 
                                 dino_features, confidence_threshold=0.7):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞—á–∞–ª—å–Ω—ã—Ö –ø—Å–µ–≤–¥–æ-–º–µ—Ç–æ–∫ —Å –≤—ã—Å–æ–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é"""
    
    pseudo_labels = {}
    high_confidence_files = []
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –Ω–µ—Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    results = baseline_model.predict(
        source=unlabeled_folder,
        conf=confidence_threshold,
        save=False,
        verbose=False
    )
    
    for result in results:
        filename = os.path.basename(result.path)
        boxes = result.boxes
        
        if boxes is not None and len(boxes) > 0:
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å DINOv2
            valid_detections = []
            
            for box in boxes:
                conf = float(box.conf[0])
                cls = int(box.cls[0])
                xyxy = box.xyxy[0].cpu().numpy()
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ DINOv2 (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
                semantic_score = check_semantic_consistency(
                    result.orig_img, xyxy, cls, dino_features[filename]
                )
                
                if semantic_score > 0.6:  # –ø–æ—Ä–æ–≥ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏
                    valid_detections.append({
                        'bbox': xyxy,
                        'class': cls,
                        'confidence': conf * semantic_score  # –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
                    })
            
            if valid_detections:
                pseudo_labels[filename] = valid_detections
                high_confidence_files.append(filename)
    
    print(f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –ø—Å–µ–≤–¥–æ-–º–µ—Ç–æ–∫ –¥–ª—è {len(pseudo_labels)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    return pseudo_labels, high_confidence_files

def check_semantic_consistency(image, bbox, predicted_class, dino_features):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ DINOv2"""
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ crop'–∞ –æ–±—ä–µ–∫—Ç–∞
    x1, y1, x2, y2 = bbox.astype(int)
    crop = image[y1:y2, x1:x2]
    
    if crop.size == 0:
        return 0.0
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ DINOv2 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ crop'–∞
    crop_pil = Image.fromarray(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    crop_tensor = transform(crop_pil).unsqueeze(0).cuda()
    
    with torch.no_grad():
        dino_model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14').cuda()
        crop_features = dino_model(crop_tensor)[0].cpu().numpy()
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –≥–ª–æ–±–∞–ª—å–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    similarity = np.dot(crop_features, dino_features) / (
        np.linalg.norm(crop_features) * np.linalg.norm(dino_features)
    )
    
    return max(0.0, similarity)

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞—á–∞–ª—å–Ω—ã—Ö –ø—Å–µ–≤–¥–æ-–º–µ—Ç–æ–∫
initial_pseudo_labels, confident_files = generate_initial_pseudo_labels(
    baseline_model, 
    "path/to/unlabeled/images",
    unlabeled_features,
    confidence_threshold=0.8
)
```

### –®–∞–≥ 3.2: –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Å–µ–≤–¥–æ-–º–µ—Ç–æ–∫
```python
def iterative_pseudo_labeling(model, unlabeled_data, labeled_data, 
                             dino_features, num_iterations=5):
    """–ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Å–µ–≤–¥–æ-–º–µ—Ç–æ–∫"""
    
    current_model = model
    iteration_results = []
    
    for iteration in range(num_iterations):
        print(f"\n=== –ò—Ç–µ—Ä–∞—Ü–∏—è {iteration + 1} ===")
        
        # 1. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Å–µ–≤–¥–æ-–º–µ—Ç–æ–∫ —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª—å—é
        confidence_thresh = 0.9 - iteration * 0.1  # –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ –ø–æ—Ä–æ–≥–∞
        
        pseudo_labels, confident_files = generate_initial_pseudo_labels(
            current_model,
            unlabeled_data,
            dino_features,
            confidence_threshold=confidence_thresh
        )
        
        print(f"–ü—Å–µ–≤–¥–æ-–º–µ—Ç–æ–∫: {len(pseudo_labels)}")
        
        # 2. –û—Ç–±–æ—Ä –ª—É—á—à–∏—Ö –ø—Å–µ–≤–¥–æ-–º–µ—Ç–æ–∫
        best_pseudo_labels = select_best_pseudo_labels(
            pseudo_labels, dino_features, top_k=min(5000, len(pseudo_labels))
        )
        
        print(f"–û—Ç–æ–±—Ä–∞–Ω–æ –ª—É—á—à–∏—Ö: {len(best_pseudo_labels)}")
        
        # 3. –°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
        extended_dataset = create_extended_dataset(
            labeled_data, best_pseudo_labels
        )
        
        # 4. –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        current_model = retrain_model(current_model, extended_dataset)
        
        # 5. –í–∞–ª–∏–¥–∞—Ü–∏—è
        val_results = current_model.val()
        iteration_results.append({
            'iteration': iteration + 1,
            'pseudo_labels_count': len(best_pseudo_labels),
            'mAP_50': val_results.box.map50,
            'mAP_50_95': val_results.box.map
        })
        
        print(f"mAP@0.5: {val_results.box.map50:.3f}")
        print(f"mAP@0.5:0.95: {val_results.box.map:.3f}")
    
    return current_model, iteration_results

def select_best_pseudo_labels(pseudo_labels, dino_features, top_k=5000):
    """–û—Ç–±–æ—Ä –ª—É—á—à–∏—Ö –ø—Å–µ–≤–¥–æ-–º–µ—Ç–æ–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏"""
    
    scored_labels = []
    
    for filename, detections in pseudo_labels.items():
        for det in detections:
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∫–æ—Ä: confidence + semantic consistency
            combined_score = det['confidence']
            
            scored_labels.append({
                'filename': filename,
                'detection': det,
                'score': combined_score
            })
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å–∫–æ—Ä–∞
    scored_labels.sort(key=lambda x: x['score'], reverse=True)
    
    # –û—Ç–±–æ—Ä top-k
    best_labels = {}
    for item in scored_labels[:top_k]:
        filename = item['filename']
        if filename not in best_labels:
            best_labels[filename] = []
        best_labels[filename].append(item['detection'])
    
    return best_labels

def create_extended_dataset(labeled_data_path, pseudo_labels):
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å –ø—Å–µ–≤–¥–æ-–º–µ—Ç–∫–∞–º–∏"""
    
    extended_path = "extended_dataset"
    os.makedirs(f"{extended_path}/images/train", exist_ok=True)
    os.makedirs(f"{extended_path}/labels/train", exist_ok=True)
    
    # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    import shutil
    shutil.copytree(f"{labeled_data_path}/images", f"{extended_path}/images", dirs_exist_ok=True)
    shutil.copytree(f"{labeled_data_path}/labels", f"{extended_path}/labels", dirs_exist_ok=True)
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Å–µ–≤–¥–æ-—Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    for filename, detections in pseudo_labels.items():
        # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        src_img = f"path/to/unlabeled/images/{filename}"
        dst_img = f"{extended_path}/images/train/{filename}"
        shutil.copy2(src_img, dst_img)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ YOLO –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
        ann_filename = filename.replace('.jpg', '.txt').replace('.png', '.txt')
        with open(f"{extended_path}/labels/train/{ann_filename}", 'w') as f:
            for det in detections:
                # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ YOLO —Ñ–æ—Ä–º–∞—Ç
                x1, y1, x2, y2 = det['bbox']
                img = cv2.imread(src_img)
                h, w = img.shape[:2]
                
                x_center = (x1 + x2) / 2 / w
                y_center = (y1 + y2) / 2 / h
                width = (x2 - x1) / w
                height = (y2 - y1) / h
                
                f.write(f"{det['class']} {x_center} {y_center} {width} {height}\n")
    
    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥–∞
    config = {
        'train': f"{extended_path}/images/train",
        'val': f"{labeled_data_path}/images/val",  # –≤–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        'nc': len(class_info['class_distribution']),
        'names': list(range(len(class_info['class_distribution'])))
    }
    
    with open(f"{extended_path}/dataset.yaml", 'w') as f:
        yaml.dump(config, f)
    
    return f"{extended_path}/dataset.yaml"

def retrain_model(model, dataset_config):
    """–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ"""
    
    # –î–æ–æ–±—É—á–µ–Ω–∏–µ —Å –º–µ–Ω—å—à–∏–º learning rate
    results = model.train(
        data=dataset_config,
        epochs=30,  # –º–µ–Ω—å—à–µ —ç–ø–æ—Ö –¥–ª—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        imgsz=640,
        batch=16,
        device=0,
        name=f'pseudo_iteration_{hash(dataset_config)}',
        resume=False,
        
        # –ú–µ–Ω—å—à–∏–π learning rate –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        lr0=0.001,
        patience=10,
        
        # –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
        hsv_h=0.015,
        hsv_s=0.7,
        hsv_v=0.4,
        degrees=5.0,
        translate=0.1,
        scale=0.3,
        fliplr=0.5,
        mosaic=0.5,  # —É–º–µ–Ω—å—à–µ–Ω–Ω–∞—è –º–æ–∑–∞–∏–∫–∞ –¥–ª—è –ø—Å–µ–≤–¥–æ-–º–µ—Ç–æ–∫
    )
    
    return model

# –ó–∞–ø—É—Å–∫ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –ø—Å–µ–≤–¥–æ-—Ä–∞–∑–º–µ—Ç–∫–∏
improved_model, iteration_history = iterative_pseudo_labeling(
    baseline_model,
    "path/to/unlabeled/images",
    "prepared_dataset",
    unlabeled_features,
    num_iterations=3  # –Ω–∞—á–Ω–∏—Ç–µ —Å 3 –∏—Ç–µ—Ä–∞—Ü–∏–π
)
```

---

## üîß **–≠–¢–ê–ü 4: Knowledge Distillation YOLOv10 ‚Üê DINOv2 (3-4 –¥–Ω—è)**

### –®–∞–≥ 4.1: –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏
```python
import torch.nn as nn
import torch.nn.functional as F

class YOLODINODistillation(nn.Module):
    """–ú–æ–¥—É–ª—å –¥–ª—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π DINOv2 ‚Üí YOLOv10"""
    
    def __init__(self, yolo_model, dino_model, distillation_layers=['backbone']):
        super().__init__()
        
        self.yolo_model = yolo_model.model  # –ø–æ–ª—É—á–∞–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω—é—é –º–æ–¥–µ–ª—å
        self.dino_model = dino_model
        self.distillation_layers = distillation_layers
        
        # –ü—Ä–æ–µ–∫—Ü–∏–æ–Ω–Ω—ã–µ —Å–ª–æ–∏ –¥–ª—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π
        self.projectors = nn.ModuleDict()
        
        # –î–ª—è backbone —É—Ä–æ–≤–Ω–µ–π YOLOv10
        backbone_dims = [256, 512, 1024]  # –ø—Ä–∏–º–µ—Ä–Ω—ã–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –¥–ª—è YOLOv10s
        dino_dim = 768  # –¥–ª—è DINOv2-B
        
        for i, dim in enumerate(backbone_dims):
            self.projectors[f'level_{i}'] = nn.Sequential(
                nn.Conv2d(dim, dino_dim, 1),
                nn.BatchNorm2d(dino_dim),
                nn.ReLU(),
                nn.Conv2d(dino_dim, dino_dim, 3, padding=1),
                nn.BatchNorm2d(dino_dim),
                nn.AdaptiveAvgPool2d(1)  # –≥–ª–æ–±–∞–ª—å–Ω—ã–π –ø—É–ª–∏–Ω–≥
            )
    
    def extract_yolo_features(self, x):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ YOLOv10"""
        features = []
        
        # –ü—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ backbone
        for i, layer in enumerate(self.yolo_model[:10]):  # backbone layers
            x = layer(x)
            if i in [4, 6, 8]:  # –ø—Ä–∏–º–µ—Ä–Ω—ã–µ —Å–ª–æ–∏ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
                features.append(x)
        
        return features
    
    def forward(self, images, targets=None):
        """–ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —Å –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–µ–π"""
        
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è YOLOv10
        yolo_outputs = self.yolo_model(images)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ YOLOv10
        yolo_features = self.extract_yolo_features(images)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ DINOv2
        with torch.no_grad():
            dino_features = self.dino_model(F.interpolate(images, size=224))
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–æ–Ω–Ω—ã—Ö –ø–æ—Ç–µ—Ä—å
        distillation_losses = {}
        
        for i, yolo_feat in enumerate(yolo_features):
            # –ü—Ä–æ–µ–∫—Ü–∏—è YOLOv10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            projected = self.projectors[f'level_{i}'](yolo_feat)
            projected = projected.view(projected.size(0), -1)  # flatten
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è
            projected_norm = F.normalize(projected, p=2, dim=1)
            dino_norm = F.normalize(dino_features, p=2, dim=1)
            
            # –î–∏—Å—Ç–∏–ª–ª—è—Ü–∏–æ–Ω–Ω–∞—è –ø–æ—Ç–µ—Ä—è
            distill_loss = 1 - F.cosine_similarity(projected_norm, dino_norm).mean()
            distillation_losses[f'distill_level_{i}'] = distill_loss
        
        if self.training and targets is not None:
            return yolo_outputs, distillation_losses
        else:
            return yolo_outputs

def train_with_distillation(yolo_model, train_dataloader, val_dataloader, 
                           dino_features_dict, num_epochs=50):
    """–û–±—É—á–µ–Ω–∏–µ YOLOv10 —Å –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–µ–π DINOv2"""
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è DINOv2
    dino_model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')
    dino_model.eval()
    dino_model.cuda()
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
    distill_model = YOLODINODistillation(yolo_model, dino_model)
    distill_model.cuda()
    distill_model.train()
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
    optimizer = torch.optim.AdamW(distill_model.parameters(), lr=1e-4, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)
    
    best_map = 0.0
    distillation_weight = 0.5  # –≤–µ—Å –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–æ–Ω–Ω–æ–π –ø–æ—Ç–µ—Ä–∏
    
    for epoch in range(num_epochs):
        distill_model.train()
        epoch_losses = {'detection': [], 'distillation': []}
        
        for batch_idx, (images, targets) in enumerate(train_dataloader):
            images = images.cuda()
            
            # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
            yolo_outputs, distill_losses = distill_model(images, targets)
            
            # –î–µ—Ç–µ–∫—Ü–∏–æ–Ω–Ω–∞—è –ø–æ—Ç–µ—Ä—è (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è YOLO –ø–æ—Ç–µ—Ä—è)
            detection_loss = compute_yolo_loss(yolo_outputs, targets)
            
            # –û–±—â–∞—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–æ–Ω–Ω–∞—è –ø–æ—Ç–µ—Ä—è
            total_distill_loss = sum(distill_losses.values()) / len(distill_losses)
            
            # –û–±—â–∞—è –ø–æ—Ç–µ—Ä—è
            total_loss = detection_loss + distillation_weight * total_distill_loss
            
            # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
            optimizer.zero_grad()
            total_loss.backward()
            optimizer.step()
            
            epoch_losses['detection'].append(detection_loss.item())
            epoch_losses['distillation'].append(total_distill_loss.item())
            
            if batch_idx % 100 == 0:
                print(f"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}: "
                      f"Det Loss: {detection_loss:.4f}, "
                      f"Distill Loss: {total_distill_loss:.4f}")
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        val_map = validate_model(distill_model, val_dataloader)
        
        print(f"Epoch {epoch+1}: "
              f"Avg Det Loss: {np.mean(epoch_losses['detection']):.4f}, "
              f"Avg Distill Loss: {np.mean(epoch_losses['distillation']):.4f}, "
              f"Val mAP: {val_map:.4f}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        if val_map > best_map:
            best_map = val_map
            torch.save(distill_model.state_dict(), 'best_distilled_model.pth')
        
        scheduler.step()
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –≤–µ—Å–∞ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏
        if epoch > num_epochs // 2:
            distillation_weight *= 0.99  # –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ —É–º–µ–Ω—å—à–µ–Ω–∏–µ
    
    return distill_model, best_map

# –ó–∞–ø—É—Å–∫ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
# –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω—É–∂–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥ –≤–∞—à –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π dataloader
print("–ù–∞—á–∏–Ω–∞–µ–º knowledge distillation...")
print("–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –ü–æ–ª–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–±—É–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å –≤–∞—à–∏–º training pipeline")
```

### –®–∞–≥ 4.2: –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ - Feature-level –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è
```python
def simpler_feature_distillation(improved_model, labeled_dataloader, 
                                unlabeled_dataloader, num_epochs=20):
    """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ DINOv2
    dino_model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')
    dino_model.eval().cuda()
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ü–∏–æ–Ω–Ω–æ–≥–æ —Å–ª–æ—è
    projector = nn.Sequential(
        nn.Linear(1000, 768),  # –æ—Ç YOLOv10 –∫ DINOv2 —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        nn.ReLU(),
        nn.Linear(768, 768)
    ).cuda()
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —Ç–æ–ª—å–∫–æ –¥–ª—è –ø—Ä–æ–µ–∫—Ü–∏–æ–Ω–Ω–æ–≥–æ —Å–ª–æ—è
    optimizer = torch.optim.Adam(
        list(improved_model.parameters()) + list(projector.parameters()),
        lr=1e-4
    )
    
    for epoch in range(num_epochs):
        total_loss = 0
        
        # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –Ω–µ—Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–µ–π
        for images in unlabeled_dataloader:
            images = images.cuda()
            
            # YOLOv10 –ø—Ä–∏–∑–Ω–∞–∫–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π)
            with torch.no_grad():
                yolo_features = improved_model.model[-2](
                    improved_model.model[:-2](images)
                ).mean(dim=[2, 3])  # global average pooling
            
            # DINOv2 –ø—Ä–∏–∑–Ω–∞–∫–∏
            with torch.no_grad():
                resized_images = F.interpolate(images, size=224)
                dino_features = dino_model(resized_images)
            
            # –ü—Ä–æ–µ–∫—Ü–∏—è YOLOv10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            projected_yolo = projector(yolo_features)
            
            # –î–∏—Å—Ç–∏–ª–ª—è—Ü–∏–æ–Ω–Ω–∞—è –ø–æ—Ç–µ—Ä—è
            distill_loss = F.mse_loss(projected_yolo, dino_features.detach())
            
            optimizer.zero_grad()
            distill_loss.backward()
            optimizer.step()
            
            total_loss += distill_loss.item()
        
        print(f"Epoch {epoch+1}: Distillation Loss = {total_loss/len(unlabeled_dataloader):.4f}")
    
    return improved_model

# –≠—Ç–æ –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞, –µ—Å–ª–∏ –ø–æ–ª–Ω–∞—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è —Å–ª–∏—à–∫–æ–º —Å–ª–æ–∂–Ω–∞
```

---

## üìä **–≠–¢–ê–ü 5: –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è (1-2 –¥–Ω—è)**

### –®–∞–≥ 5.1: –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
```python
def comprehensive_evaluation(models_dict, test_dataloader):
    """–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
    
    results = {}
    
    for model_name, model in models_dict.items():
        print(f"\n–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏: {model_name}")
        
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏
        val_results = model.val()
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        additional_metrics = compute_additional_metrics(model, test_dataloader)
        
        results[model_name] = {
            'mAP_50': val_results.box.map50,
            'mAP_50_95': val_results.box.map,
            'precision': val_results.box.p,
            'recall': val_results.box.r,
            **additional_metrics
        }
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤–æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã
    import pandas as pd
    
    df_results = pd.DataFrame(results).T
    print("\n=== –°–í–û–î–ù–ê–Ø –¢–ê–ë–õ–ò–¶–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ===")
    print(df_results.round(4))
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    df_results.to_csv('model_comparison_results.csv')
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    plot_results_comparison(df_results)
    
    return results

def compute_additional_metrics(model, dataloader):
    """–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏"""
    
    all_predictions = []
    inference_times = []
    
    model.eval()
    with torch.no_grad():
        for images, targets in dataloader:
            start_time = time.time()
            
            predictions = model(images)
            
            inference_time = (time.time() - start_time) / len(images)
            inference_times.append(inference_time)
            
            all_predictions.extend(predictions)
    
    return {
        'avg_inference_time_ms': np.mean(inference_times) * 1000,
        'std_inference_time_ms': np.std(inference_times) * 1000,
        'total_predictions': len(all_predictions)
    }

def plot_results_comparison(df_results):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # mAP@0.5
    axes[0, 0].bar(df_results.index, df_results['mAP_50'])
    axes[0, 0].set_title('mAP@0.5')
    axes[0, 0].set_ylabel('mAP')
    axes[0, 0].tick_params(axis='x', rotation=45)
    
    # mAP@0.5:0.95
    axes[0, 1].bar(df_results.index, df_results['mAP_50_95'])
    axes[0, 1].set_title('mAP@0.5:0.95')
    axes[0, 1].set_ylabel('mAP')
    axes[0, 1].tick_params(axis='x', rotation=45)
    
    # Precision vs Recall
    axes[1, 0].scatter(df_results['recall'], df_results['precision'], s=100)
    for i, model in enumerate(df_results.index):
        axes[1, 0].annotate(model, (df_results['recall'].iloc[i], 
                                   df_results['precision'].iloc[i]))
    axes[1, 0].set_xlabel('Recall')
    axes[1, 0].set_ylabel('Precision')
    axes[1, 0].set_title('Precision vs Recall')
    
    # –í—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
    axes[1, 1].bar(df_results.index, df_results['avg_inference_time_ms'])
    axes[1, 1].set_title('–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ (–º—Å)')
    axes[1, 1].set_ylabel('–í—Ä–µ–º—è (–º—Å)')
    axes[1, 1].tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    plt.savefig('model_comparison.png', dpi=150, bbox_inches='tight')
    plt.show()

# –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
models_to_evaluate = {
    'baseline_yolov10': baseline_model,
    'pseudo_labeled': improved_model,
    # 'distilled': distilled_model,  # –µ—Å–ª–∏ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è
}

final_results = comprehensive_evaluation(models_to_evaluate, test_dataloader)
```

### –®–∞–≥ 5.2: –í—ã–±–æ—Ä –∏ —ç–∫—Å–ø–æ—Ä—Ç –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
```python
def select_and_export_best_model(results_dict, models_dict, 
                                main_metric='mAP_50', 
                                secondary_metric='mAP_50_95'):
    """–í—ã–±–æ—Ä –∏ —ç–∫—Å–ø–æ—Ä—Ç –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏"""
    
    # –í—ã–±–æ—Ä –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
    best_model_name = max(results_dict.keys(), 
                         key=lambda x: results_dict[x][main_metric])
    
    best_model = models_dict[best_model_name]
    best_score = results_dict[best_model_name][main_metric]
    
    print(f"\n–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_name}")
    print(f"{main_metric}: {best_score:.4f}")
    
    # –≠–∫—Å–ø–æ—Ä—Ç –≤ —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
    export_formats = ['onnx', 'torchscript', 'tflite']
    
    for format_name in export_formats:
        try:
            export_path = f"best_model_{best_model_name}.{format_name}"
            
            if format_name == 'onnx':
                best_model.export(format='onnx', optimize=True)
            elif format_name == 'torchscript':
                best_model.export(format='torchscript')
            elif format_name == 'tflite':
                best_model.export(format='tflite')
            
            print(f"–ú–æ–¥–µ–ª—å —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ –≤ {format_name}: {export_path}")
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ {format_name}: {e}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏ –º–µ—Ç—Ä–∏–∫
    model_config = {
        'best_model_name': best_model_name,
        'metrics': results_dict[best_model_name],
        'training_details': {
            'labeled_samples': 10000,
            'unlabeled_samples': 300000,
            'training_strategy': 'YOLOv10 + DINOv2 + Pseudo-labeling',
            'final_dataset_size': 'labeled + selected_pseudo_labeled'
        }
    }
    
    import json
    with open('best_model_config.json', 'w') as f:
        json.dump(model_config, f, indent=2)
    
    return best_model, best_model_name

# –í—ã–±–æ—Ä –∏ —ç–∫—Å–ø–æ—Ä—Ç –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
best_model, best_name = select_and_export_best_model(
    final_results, models_to_evaluate
)
```

---

## üìã **–†–ï–ó–Æ–ú–ï –ò –í–†–ï–ú–ï–ù–ù–´–ï –ó–ê–¢–†–ê–¢–´**

### –û–±—â–∏–π timeline (8-12 –¥–Ω–µ–π):
```
–î–µ–Ω—å 1-2:   –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö + –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞
–î–µ–Ω—å 3-4:   Baseline YOLOv10 –æ–±—É—á–µ–Ω–∏–µ  
–î–µ–Ω—å 5:     –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ DINOv2 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
–î–µ–Ω—å 6-8:   –ü—Å–µ–≤–¥–æ-—Ä–∞–∑–º–µ—Ç–∫–∞ (–∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–∞—è)
–î–µ–Ω—å 9-11:  Knowledge Distillation (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
–î–µ–Ω—å 12:    –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ + —ç–∫—Å–ø–æ—Ä—Ç
```

### –û–∂–∏–¥–∞–µ–º—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:
- **Baseline YOLOv10**: mAP@0.5 ~ 0.65-0.75
- **+ –ü—Å–µ–≤–¥–æ-—Ä–∞–∑–º–µ—Ç–∫–∞**: +3-7% mAP
- **+ –î–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è**: +1-3% mAP

### –ö–ª—é—á–µ–≤—ã–µ —Ñ–∞–π–ª—ã –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è:
```
best_model_[name].pt          # –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å
best_model_config.json        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
model_comparison_results.csv  # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
labeled_dino_features.npy     # DINOv2 –ø—Ä–∏–∑–Ω–∞–∫–∏ (—Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ)
unlabeled_dino_features.npy   # DINOv2 –ø—Ä–∏–∑–Ω–∞–∫–∏ (–Ω–µ—Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ)
```

### –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã:
1. **–ù–∞—á–Ω–∏—Ç–µ —Å –ø—Ä–æ—Å—Ç–æ–≥–æ**: —Å–Ω–∞—á–∞–ª–∞ baseline, –∑–∞—Ç–µ–º –ø—Å–µ–≤–¥–æ-—Ä–∞–∑–º–µ—Ç–∫–∞
2. **–ú–æ–Ω–∏—Ç–æ—Ä—å—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ**: —Ä–µ–≥—É–ª—è—Ä–Ω–æ –ø—Ä–æ–≤–µ—Ä—è–π—Ç–µ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ
3. **–≠–∫–æ–Ω–æ–º—å—Ç–µ –≤—Ä–µ–º—è**: –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞, –æ—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–∏—Ä–æ—Å—Ç –æ—Ç –ø—Å–µ–≤–¥–æ-—Ä–∞–∑–º–µ—Ç–∫–∏
4. **–°–æ—Ö—Ä–∞–Ω—è–π—Ç–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã**: –∫–∞–∂–¥—ã–π —ç—Ç–∞–ø –º–æ–∂–µ—Ç –ø—Ä–∏–≥–æ–¥–∏—Ç—å—Å—è
5. **–ê–¥–∞–ø—Ç–∏—Ä—É–π—Ç–µ –ø–æ–¥ —Ä–µ—Å—É—Ä—Å—ã**: batch_size –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –ø–æ–¥ –≤–∞—à—É GPU

**–ì–æ—Ç–æ–≤ –ø–æ–º–æ—á—å —Å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–µ–π –ª—é–±–æ–≥–æ –∏–∑ —ç—Ç–∞–ø–æ–≤! üöÄ**