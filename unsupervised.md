# üîç –î–µ—Ç–∞–ª—å–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ Domain Adaptation –¥–ª—è DINOv2

–†–∞–∑–±–µ—Ä—É **–ø–æ—à–∞–≥–æ–≤–æ**, —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –∏ **–ø–æ—á–µ–º—É** —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ.

---

## üéØ **–ß–¢–û –ü–†–û–ò–°–•–û–î–ò–¢: –ü–æ—à–∞–≥–æ–≤—ã–π —Ä–∞–∑–±–æ—Ä**

### –®–∞–≥ 1: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
```python
base_features = dinov2_model(domain_images)  # [N, 768]
```

#### –ß—Ç–æ –¥–µ–ª–∞–µ—Ç—Å—è:
- –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ **–∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—É—é** DINOv2
- –ü–æ–ª—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä—ã —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ 768 –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
- **DINOv2 –æ—Å—Ç–∞–µ—Ç—Å—è –Ω–µ–∏–∑–º–µ–Ω–Ω–æ–π** - –≤–µ—Å–∞ –Ω–µ –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è

#### –ß—Ç–æ –ø–æ–ª—É—á–∞–µ–º:
```python
# –ü—Ä–∏–º–µ—Ä –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö —Å–Ω–∏–º–∫–æ–≤:
chest_xray = load_image("chest_xray.jpg")
dinov2_features = dinov2_model(chest_xray)  # [1, 768]

# –≠—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç:
general_concepts = [
    "–∫—Ä–∞—è –∏ –∫–æ–Ω—Ç—É—Ä—ã",           # DINOv2 —Ö–æ—Ä–æ—à–æ –≤–∏–¥–∏—Ç
    "—Ç–µ–∫—Å—Ç—É—Ä—ã –∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã",      # DINOv2 —Ö–æ—Ä–æ—à–æ –≤–∏–¥–∏—Ç  
    "–æ–±—â–∏–µ —Ñ–æ—Ä–º—ã",              # DINOv2 —Ö–æ—Ä–æ—à–æ –≤–∏–¥–∏—Ç
    "–∞–Ω–∞—Ç–æ–º–∏—á–µ—Å–∫–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã",  # DINOv2 –ù–ï –ø–æ–Ω–∏–º–∞–µ—Ç —Å–ø–µ—Ü–∏—Ñ–∏–∫—É
    "–ø–∞—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è"  # DINOv2 –ù–ï –ø–æ–Ω–∏–º–∞–µ—Ç –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
]
```

### –®–∞–≥ 2: Domain-specific –ø—Ä–æ–µ–∫—Ç–æ—Ä
```python
domain_projector = nn.Sequential(
    nn.Linear(768, 512),    # –°–∂–∏–º–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
    nn.ReLU(),              # –ù–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å
    nn.Linear(512, 768),    # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
    nn.LayerNorm(768)       # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
)
```

#### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–æ—Ä–∞:
```python
# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏:
input_features:  [0.2, -0.5, 0.8, ..., 0.1]  # 768 —á–∏—Å–µ–ª (–æ–±—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏)
         ‚Üì
Linear(768‚Üí512): [0.4, -0.1, 0.6, ..., 0.3]  # 512 —á–∏—Å–µ–ª (—Å–∂–∞—Ç–∏–µ)
         ‚Üì
ReLU():          [0.4, 0.0, 0.6, ..., 0.3]   # –£–±–∏—Ä–∞–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ
         ‚Üì  
Linear(512‚Üí768): [0.1, -0.3, 0.9, ..., 0.7]  # 768 —á–∏—Å–µ–ª (–≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ)
         ‚Üì
LayerNorm():     [0.0, -0.2, 1.2, ..., 0.8]  # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
```

#### **–ó–∞—á–µ–º —Ç–∞–∫–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞?**

1. **–°–∂–∞—Ç–∏–µ (768‚Üí512)**: –∑–∞—Å—Ç–∞–≤–ª—è–µ—Ç —Å–µ—Ç—å –≤—ã–¥–µ–ª–∏—Ç—å **—Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ** –ø—Ä–∏–∑–Ω–∞–∫–∏
```python
# Bottleneck —ç—Ñ—Ñ–µ–∫—Ç:
all_features = ["–∫—Ä–∞–π1", "—Ç–µ–∫—Å—Ç—É—Ä–∞1", "—Ñ–æ—Ä–º–∞1", ..., "–ø–∞—Ç—Ç–µ—Ä–Ω768"]
compressed = select_most_important(all_features, top_k=512)
# –°–µ—Ç—å —É—á–∏—Ç—Å—è –≤—ã–±–∏—Ä–∞—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–ª—è –¥–æ–º–µ–Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏
```

2. **–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ (512‚Üí768)**: –¥–æ–±–∞–≤–ª—è–µ—Ç **domain-specific** –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
```python
# –ü—Ä–∏–º–µ—Ä –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö —Å–Ω–∏–º–∫–æ–≤:
general_edge = 0.5         # "–∫–∞–∫–æ–π-—Ç–æ –∫—Ä–∞–π" (–æ—Ç DINOv2)
‚Üì (domain projector)
medical_rib_edge = 0.8     # "–∫—Ä–∞–π —Ä–µ–±—Ä–∞" (–∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–æ –ø–æ–¥ –¥–æ–º–µ–Ω)
```

### –®–∞–≥ 3: Contrastive Learning –æ–±—É—á–µ–Ω–∏–µ
```python
def train_domain_projector(projector, dinov2_model, domain_dataloader):
    """–û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–æ—Ä–∞ –Ω–∞ contrastive learning"""
    
    optimizer = torch.optim.Adam(projector.parameters(), lr=1e-3)
    
    for batch in domain_dataloader:
        images = batch  # [batch_size, 3, 224, 224]
        
        # 1. –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –ø–∞—Ä
        aug1 = strong_augment(images)  # –ü–µ—Ä–≤–∞—è –≤–µ—Ä—Å–∏—è
        aug2 = strong_augment(images)  # –í—Ç–æ—Ä–∞—è –≤–µ—Ä—Å–∏—è —Ç–æ–≥–æ –∂–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        
        # 2. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (DINOv2 –∑–∞–º–æ—Ä–æ–∂–µ–Ω–∞!)
        with torch.no_grad():
            base_features1 = dinov2_model(aug1)  # [batch, 768]
            base_features2 = dinov2_model(aug2)  # [batch, 768]
        
        # 3. –ü—Ä–æ–µ–∫—Ü–∏—è –≤ domain-adapted –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ
        adapted1 = projector(base_features1)     # [batch, 768]
        adapted2 = projector(base_features2)     # [batch, 768]
        
        # 4. Contrastive loss
        loss = contrastive_loss(adapted1, adapted2)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    return projector
```

#### **–ß—Ç–æ —Ç–∞–∫–æ–µ Contrastive Learning –∑–¥–µ—Å—å?**
```python
def contrastive_loss(features1, features2, temperature=0.1):
    """
    –¶–µ–ª—å: –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø–æ—Ö–æ–∂–∏,
    —Ä–∞–∑–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è - –æ—Ç–ª–∏—á–∞—Ç—å—Å—è
    """
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è cosine similarity
    features1 = F.normalize(features1, dim=1)  
    features2 = F.normalize(features2, dim=1)
    
    # Similarity –º–∞—Ç—Ä–∏—Ü–∞
    similarity_matrix = torch.mm(features1, features2.T) / temperature
    
    # Positive pairs = –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã (aug1[i] —Å aug2[i])
    positive_samples = torch.diag(similarity_matrix)
    
    # Negative pairs = off-diagonal —ç–ª–µ–º–µ–Ω—Ç—ã  
    negative_samples = similarity_matrix - torch.diag(torch.diag(similarity_matrix))
    
    # InfoNCE loss
    loss = -torch.log(
        torch.exp(positive_samples) / 
        (torch.exp(positive_samples) + torch.sum(torch.exp(negative_samples), dim=1))
    ).mean()
    
    return loss
```

---

## üéØ **–ó–ê–ß–ï–ú –≠–¢–û –î–ï–õ–ê–ï–¢–°–Ø: –ì–ª—É–±–æ–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ**

### üß† **–ü—Ä–æ–±–ª–µ–º–∞ 1: DINOv2 "–Ω–µ –∑–Ω–∞–µ—Ç" –≤–∞—à –¥–æ–º–µ–Ω**
```python
# –ü—Ä–∏–º–µ—Ä: –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ —Ä–µ–Ω—Ç–≥–µ–Ω–æ–≤—Å–∫–∏–µ —Å–Ω–∏–º–∫–∏
original_dinov2_understanding = {
    "—Ä–µ–±—Ä–æ": "–ø—Ä–æ—Å—Ç–æ —Å–≤–µ—Ç–ª–∞—è –ª–∏–Ω–∏—è",
    "–ª–µ–≥–∫–æ–µ": "—Ç–µ–º–Ω–∞—è –æ–±–ª–∞—Å—Ç—å —Å –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏", 
    "–ø–∞—Ç–æ–ª–æ–≥–∏—è": "–Ω–µ–ø–æ–Ω—è—Ç–Ω–æ–µ –ø—è—Ç–Ω–æ",
    "–∞–Ω–∞—Ç–æ–º–∏—è": "–Ω–∞–±–æ—Ä –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã—Ö —Ñ–æ—Ä–º"
}

# –ü–æ—Å–ª–µ domain adaptation:
adapted_understanding = {
    "—Ä–µ–±—Ä–æ": "–∞–Ω–∞—Ç–æ–º–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º–∏ —Å–≤–æ–π—Å—Ç–≤–∞–º–∏",
    "–ª–µ–≥–∫–æ–µ": "–æ—Ä–≥–∞–Ω —Å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω–æ–π —Ç–µ–∫—Å—Ç—É—Ä–æ–π –∏ –≥—Ä–∞–Ω–∏—Ü–∞–º–∏",
    "–ø–∞—Ç–æ–ª–æ–≥–∏—è": "–æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—Ç –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π –∞–Ω–∞—Ç–æ–º–∏–∏", 
    "–∞–Ω–∞—Ç–æ–º–∏—è": "—Å–≤—è–∑–∞–Ω–Ω—ã–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"
}
```

### üîß **–ú–µ—Ö–∞–Ω–∏–∑–º —Ä–∞–±–æ—Ç—ã –ø—Ä–æ–µ–∫—Ç–æ—Ä–∞**

#### **–î–æ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏:**
```python
# DINOv2 –≤–∏–¥–∏—Ç —Ä–µ–Ω—Ç–≥–µ–Ω –∫–∞–∫ "–∞–±—Å—Ç—Ä–∞–∫—Ç–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"
dinov2_features = [
    0.2,  # "–µ—Å—Ç—å –∫–∞–∫–∞—è-—Ç–æ –ª–∏–Ω–∏—è"
    -0.5, # "—Ç–µ–º–Ω–∞—è –æ–±–ª–∞—Å—Ç—å —Å–ª–µ–≤–∞"
    0.8,  # "–∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–∞—è –≥—Ä–∞–Ω–∏—Ü–∞"
    ...
]
# –ö–æ–Ω—Ç–µ–∫—Å—Ç: –û–ë–©–ò–ï –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
```

#### **–ü–æ—Å–ª–µ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏:**
```python  
# –ü—Ä–æ–µ–∫—Ç–æ—Ä "–ø–µ—Ä–µ–≤–æ–¥–∏—Ç" –≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
adapted_features = [
    0.7,  # "–ª–∏–Ω–∏—è" ‚Üí "—Ä–µ–±—Ä–æ" (—É—Å–∏–ª–∏–ª–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å)
    -0.1, # "—Ç–µ–º–Ω–∞—è –æ–±–ª–∞—Å—Ç—å" ‚Üí "–Ω–æ—Ä–º–∞–ª—å–Ω–∞—è –ª–µ–≥–æ—á–Ω–∞—è —Ç–∫–∞–Ω—å" 
    1.2,  # "–≥—Ä–∞–Ω–∏—Ü–∞" ‚Üí "–∞–Ω–∞—Ç–æ–º–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞"
    ...
]
# –ö–æ–Ω—Ç–µ–∫—Å—Ç: –ú–ï–î–ò–¶–ò–ù–°–ö–ò–ï –ø–∞—Ç—Ç–µ—Ä–Ω—ã
```

### üé™ **–ü–æ—á–µ–º—É bottleneck architecture —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞?**

```python
# –ê–Ω–∞–ª–æ–≥–∏—è —Å –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–æ–º:
english_sentence = "The patient has a fracture in the rib"
                        ‚Üì (compress to meaning)
core_meaning = ["patient", "fracture", "rib"]  # –ö–ª—é—á–µ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                        ‚Üì (expand to target language)
russian_sentence = "–£ –ø–∞—Ü–∏–µ–Ω—Ç–∞ –ø–µ—Ä–µ–ª–æ–º —Ä–µ–±—Ä–∞"

# –í –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ:
general_features = dinov2_output              # "–û–±—â–∏–π –≤–∏–∑—É–∞–ª—å–Ω—ã–π —è–∑—ã–∫"
                        ‚Üì (768 ‚Üí 512: extract core)
core_concepts = most_important_patterns       # –ö–ª—é—á–µ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
                        ‚Üì (512 ‚Üí 768: adapt to domain)  
domain_features = medical_context_features    # "–ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π —è–∑—ã–∫"
```

---

## üöÄ **–ü–†–ê–ö–¢–ò–ß–ï–°–ö–ò–ï –ü–†–ï–ò–ú–£–©–ï–°–¢–í–ê**

### ‚úÖ **1. –°–∫–æ—Ä–æ—Å—Ç—å**
```python
# –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è:
full_dinov2_finetune = {
    "parameters": "86M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ DINOv2",
    "time": "3-5 –¥–Ω–µ–π –Ω–∞ A100",
    "data_requirement": "–º–Ω–æ–≥–æ GPU –ø–∞–º—è—Ç–∏"
}

domain_adaptation = {
    "parameters": "1.2M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø—Ä–æ–µ–∫—Ç–æ—Ä–∞ (–≤ 70 —Ä–∞–∑ –º–µ–Ω—å—à–µ!)",
    "time": "4-8 —á–∞—Å–æ–≤ –Ω–∞ –æ–¥–Ω–æ–π GPU", 
    "data_requirement": "–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã"
}
```

### ‚úÖ **2. –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å**
```python
# –†–∏—Å–∫–∏:
full_finetune_risks = [
    "catastrophic forgetting",  # DINOv2 –∑–∞–±—ã–≤–∞–µ—Ç –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è
    "overfitting",             # –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –≤–∞—à–µ–º –¥–æ–º–µ–Ω–µ
    "degraded performance"     # —Ö—É–∂–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –¥—Ä—É–≥–∏—Ö –¥–∞–Ω–Ω—ã—Ö
]

domain_adaptation_risks = [
    # –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç!
    # DINOv2 –æ—Å—Ç–∞–µ—Ç—Å—è –Ω–µ—Ç—Ä–æ–Ω—É—Ç–æ–π
    "minimal overfitting risk" # —Ç–æ–ª—å–∫–æ –º–∞–ª–µ–Ω—å–∫–∏–π –ø—Ä–æ–µ–∫—Ç–æ—Ä –º–æ–∂–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–∏—Ç—å—Å—è
]
```

### ‚úÖ **3. –ì–∏–±–∫–æ—Å—Ç—å**
```python
# –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –û–î–ù–£ DINOv2 –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤:
shared_dinov2 = load_pretrained_dinov2()

medical_projector = train_domain_adapter(medical_data)
satellite_projector = train_domain_adapter(satellite_data)  
industrial_projector = train_domain_adapter(industrial_data)

# –í inference:
medical_features = medical_projector(shared_dinov2(medical_image))
satellite_features = satellite_projector(shared_dinov2(satellite_image))
```

---

## üìä **–ü–û–õ–ù–´–ô –ö–û–î –†–ï–ê–õ–ò–ó–ê–¶–ò–ò**

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import torchvision.transforms as transforms

class DomainAdapter(nn.Module):
    """Domain adaptation —Å–ª–æ–π –¥–ª—è DINOv2"""
    
    def __init__(self, input_dim=768, bottleneck_dim=512):
        super().__init__()
        
        self.projector = nn.Sequential(
            # Encoder: —Å–∂–∏–º–∞–µ–º –¥–æ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤
            nn.Linear(input_dim, bottleneck_dim),
            nn.BatchNorm1d(bottleneck_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            
            # Decoder: —Ä–∞—Å—à–∏—Ä—è–µ–º —Å domain knowledge
            nn.Linear(bottleneck_dim, input_dim),
            nn.BatchNorm1d(input_dim),
            nn.Tanh()  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤—ã—Ö–æ–¥–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        )
        
        # Residual connection –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        self.residual_weight = nn.Parameter(torch.tensor(0.1))
        
    def forward(self, dinov2_features):
        # –û—Å–Ω–æ–≤–Ω–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è
        adapted = self.projector(dinov2_features)
        
        # Residual connection: —Å–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞—Å—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π
        output = adapted + self.residual_weight * dinov2_features
        
        return F.normalize(output, dim=1)  # L2 –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è

class ContrastiveLearningTrainer:
    """–û–±—É—á–µ–Ω–∏–µ domain adapter —á–µ—Ä–µ–∑ contrastive learning"""
    
    def __init__(self, dinov2_model, domain_adapter, temperature=0.1):
        self.dinov2 = dinov2_model
        self.dinov2.eval()  # –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º DINOv2!
        
        self.adapter = domain_adapter
        self.temperature = temperature
        
        # –°–∏–ª—å–Ω—ã–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä
        self.strong_aug = transforms.Compose([
            transforms.RandomResizedCrop(224, scale=(0.3, 1.0)),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.ColorJitter(brightness=0.4, contrast=0.4, 
                                 saturation=0.4, hue=0.1),
            transforms.RandomGrayscale(p=0.2),
            transforms.RandomRotation(15),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                               std=[0.229, 0.224, 0.225])
        ])
    
    def contrastive_loss(self, features1, features2):
        """InfoNCE contrastive loss"""
        batch_size = features1.shape[0]
        
        # Concatenate –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ–ª–Ω–æ–π –º–∞—Ç—Ä–∏—Ü—ã similarity
        features = torch.cat([features1, features2], dim=0)  # [2*batch, 768]
        
        # Compute similarity matrix  
        sim_matrix = torch.mm(features, features.T) / self.temperature  # [2*batch, 2*batch]
        
        # –ú–∞—Å–∫–∏—Ä—É–µ–º –¥–∏–∞–≥–æ–Ω–∞–ª—å (—Å–∞–º–æ—Å—Ö–æ–¥—Å—Ç–≤–æ)
        mask = torch.eye(2 * batch_size, dtype=torch.bool, device=features.device)
        sim_matrix.masked_fill_(mask, -float('inf'))
        
        # Positive pairs: (i, i+batch) –∏ (i+batch, i)
        positive_indices = torch.arange(batch_size, device=features.device)
        pos_sim1 = sim_matrix[positive_indices, positive_indices + batch_size]  
        pos_sim2 = sim_matrix[positive_indices + batch_size, positive_indices]
        
        # Negative samples: –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ
        neg_sim1 = sim_matrix[positive_indices]  # [batch, 2*batch]
        neg_sim2 = sim_matrix[positive_indices + batch_size]  # [batch, 2*batch]
        
        # InfoNCE loss
        loss1 = -torch.log(torch.exp(pos_sim1) / torch.sum(torch.exp(neg_sim1), dim=1))
        loss2 = -torch.log(torch.exp(pos_sim2) / torch.sum(torch.exp(neg_sim2), dim=1))
        
        return (loss1.mean() + loss2.mean()) / 2
    
    def train_epoch(self, dataloader, optimizer):
        """–û–¥–Ω–∞ —ç–ø–æ—Ö–∞ –æ–±—É—á–µ–Ω–∏—è"""
        self.adapter.train()
        total_loss = 0
        
        for batch_idx, images in enumerate(dataloader):
            images = images.cuda()
            
            # –°–æ–∑–¥–∞–µ–º –¥–≤–∞ augmented –≤–∞—Ä–∏–∞–Ω—Ç–∞
            aug1 = torch.stack([self.strong_aug(img) for img in images])
            aug2 = torch.stack([self.strong_aug(img) for img in images])
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∞–∑–æ–≤—ã–µ DINOv2 –ø—Ä–∏–∑–Ω–∞–∫–∏ (–±–µ–∑ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤!)
            with torch.no_grad():
                dinov2_feat1 = self.dinov2(aug1.cuda())
                dinov2_feat2 = self.dinov2(aug2.cuda())
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º domain adaptation
            adapted1 = self.adapter(dinov2_feat1)
            adapted2 = self.adapter(dinov2_feat2)
            
            # Contrastive loss
            loss = self.contrastive_loss(adapted1, adapted2)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            
            if batch_idx % 50 == 0:
                print(f'Batch {batch_idx}: Loss = {loss.item():.4f}')
        
        return total_loss / len(dataloader)

def train_domain_adapter(domain_images_folder, num_epochs=20):
    """–ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è domain adapter"""
    
    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º DINOv2 (–∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—É—é)
    dinov2_model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')
    dinov2_model.cuda().eval()
    
    # 2. –°–æ–∑–¥–∞–µ–º domain adapter
    adapter = DomainAdapter(input_dim=768, bottleneck_dim=512).cuda()
    
    # 3. –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    from torchvision.datasets import ImageFolder
    dataset = ImageFolder(domain_images_folder)  # –¢–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –º–µ—Ç–∫–∏ –Ω–µ –Ω—É–∂–Ω—ã
    dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=4)
    
    # 4. Trainer –∏ optimizer
    trainer = ContrastiveLearningTrainer(dinov2_model, adapter)
    optimizer = torch.optim.AdamW(adapter.parameters(), lr=1e-3, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)
    
    # 5. –û–±—É—á–µ–Ω–∏–µ
    best_loss = float('inf')
    
    for epoch in range(num_epochs):
        avg_loss = trainer.train_epoch(dataloader, optimizer)
        scheduler.step()
        
        print(f'Epoch {epoch+1}/{num_epochs}: Avg Loss = {avg_loss:.4f}')
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        if avg_loss < best_loss:
            best_loss = avg_loss
            torch.save(adapter.state_dict(), f'domain_adapter_best.pth')
    
    return adapter

# 6. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π DINOv2
def extract_adapted_features(dinov2_model, adapter, images):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ domain-adapted –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    adapter.eval()
    
    with torch.no_grad():
        # –ë–∞–∑–æ–≤—ã–µ DINOv2 –ø—Ä–∏–∑–Ω–∞–∫–∏
        base_features = dinov2_model(images)
        
        # Domain adaptation
        adapted_features = adapter(base_features)
    
    return adapted_features.cpu().numpy()
```

---

## üéØ **–ò–¢–û–ì–û–í–û–ï –°–†–ê–í–ù–ï–ù–ò–ï**

| –ü–æ–¥—Ö–æ–¥ | –í—Ä–µ–º—è | GPU –ø–∞–º—è—Ç—å | –†–∏—Å–∫ | –ü—Ä–∏—Ä–æ—Å—Ç –∫–∞—á–µ—Å—Ç–≤–∞ |
|--------|-------|------------|------|------------------|
| **Original DINOv2** | 0 –¥–Ω–µ–π | 0 | –ù–µ—Ç | Baseline |
| **Domain Adaptation** | 1 –¥–µ–Ω—å | 8GB | –ù–∏–∑–∫–∏–π | **+3-8%** |  
| **Full Finetune** | 5 –¥–Ω–µ–π | 32GB | –í—ã—Å–æ–∫–∏–π | +5-15% |

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è**: –Ω–∞—á–∏–Ω–∞–π—Ç–µ —Å Domain Adaptation - —ç—Ç–æ **sweet spot** –º–µ–∂–¥—É —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å—é! üöÄ

**Domain Adaptation = 80% –ø–æ–ª—å–∑—ã –æ—Ç finetune –∑–∞ 20% —É—Å–∏–ª–∏–π** üí™